{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Linear Algebra\n",
    "https://nbviewer.jupyter.org/github/trungmanhhuynh/back_ground/blob/master/linear_algebra.ipynb?flush_cache=true\n",
    "\n",
    "# Resources: \n",
    "\n",
    "1. Ian Goodfellow presented a amazing summary of linear algebra for machine learning in a chapter of his book. [pdf](http://www.deeplearningbook.org/contents/linear_algebra.html)\n",
    "\n",
    "2. Elementary Linear Algebra_Ron_Larson (https://bonniekhanhtran.files.wordpress.com/2016/05/math-g235.pdf)\n",
    "\n",
    "3. MIT Algebra Spring 2015, https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8\n",
    "\n",
    "# Matrix Transpose \n",
    "\n",
    "<img src=\"./images/linear_fig1.JPG\" width=\"500\">\n",
    "\n",
    "# Matrix Inverse \n",
    "\n",
    "**Matrix $A$ is invertible (nonsingular) if A is square matrix and consists of linearly independent collumns. (A matrix is called singular if it consists of linearly dependent collumns).**\n",
    "\n",
    " **Proof 1:** We can think that when $A (mxn)$ is invertible then the linear equation $Ax =b$ does have only solution $x =A^{-1}b$. To achieve this, we can think of the span of column vectors of A in collumn space. Since $Ax = a_{:,1}x_1 + a_{:,2}x_2 + a_{:,n} x_n $, where $a_{:,i}$ is $i^{th}$ column of matrix $A$, $x_i$ is the $i^{th}$ element of vector $x$, $Ax$ is the linear combination of all collums of $A$ and $x$ is coefficents. There is two requirements:\n",
    " \n",
    "  a) Columns of A must span the space $\\mathbb{R}^{m}$ meaning that every points $b \\in \\mathbb{R}^{m}$, can be described as a linear combination with some coefficients $x$ of columns of $A$. Thus, all collums of $A$ must be linearly independent and the number of columns $n$ of $A$ must be equal or more than number of rows $m$ ( $n \\geq m$ ).\n",
    "  \n",
    "  b) Since there is only one solution $x$, $A$ must be square matrix ($m = n$), else more than one solution exists because there exists more than one set of linearly independent in $A$. This could be also be explain that if A is not square matrix $AB \\neq BA$, this contradicts the definition of the inverse of matrix$.\n",
    "  \n",
    " **How to find inverse of a matrix?** \n",
    "  <img src=\"./images/linear_inverse_1.JPG\" width=\"500\">\n",
    "\n",
    "  \n",
    " **Theorems**\n",
    " 1. If A and B are invertible, then $(AB)^{-1} = B^{-1}A^{-1}$\n",
    " 1. $(cA)^{-1} = 1/cA^{-1}$\n",
    " 1. $(A^T)^{-1} = (A^{-1})^T$\n",
    " 1. $(A^k)^{-1} = (A^{-1})^k$\n",
    "\n",
    "# MIT Algebra Spring 2015\n",
    "<h2> Lecture 1: The Geometry of Linear Equations. </h2>\n",
    "\n",
    "When solving linear equation $Ax = b$, we can think of $AX$, where is a matrix $m \\times n$, and $X$is a vector $n x 1$ as a row picture or collumn picture. \n",
    "\n",
    "For example, Ax = $\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$ $\\begin{bmatrix} x \\\\ y \\end{bmatrix}$ =  $\\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}$\n",
    "\n",
    "In **row picture** $ X = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$ is the intersection (if exists) of two lines $2x -y = 0$ and $-x + 2y =3$ in $ \\mathbb{R}^{2}$.\n",
    "\n",
    "In **collum picture** $b$ is  a linear combination of columns of A. We can write:\n",
    "$ Ax = x\\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} + y \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix} =\\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}$, \n",
    "where each column of A if a vector in $\\mathbb{R}^{2}$. Based on this intuition, if columns of A are linearly independent, then all combinations of them covers entire $ \\mathbb{R}^{2}$ space. It means that there always exists a soluton $X$ so that the linear combination $AX =b$\n",
    "\n",
    "<h2> Lecture 2: Elimination of Matrices. </h2>\n",
    "\n",
    "Gauss or Gauss-Jordan algorithm can be used to solve $AX =b $ using eliminations. By applying three followings elementary operations, one can transform any matrix to row-echelon form (Gauss) and reduced row-echelon form (Gauss-Jordan):\n",
    "- Interchange two rows \n",
    "- Multiply a row by a nonzero constant\n",
    "- Add a multiple of a row to another row\n",
    "\n",
    "Remember, these operations must be done for both sides of an equation.\n",
    "  <img src=\"./images/linear_row_echelon.JPG\" width=\"500\">\n",
    " \n",
    "For example:\n",
    "\n",
    "$A = \\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 8 & 1 \\\\ 0 & 4 & 1  \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 2 & -2\\\\ 0 & 4 & 1   \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 2 & -2\\\\ 0 & 0 & 5  \\end{bmatrix}$ (Row-echelon form).\n",
    "Now, using back-subtitution, we can solve for $X$\n",
    "\n",
    "Interestingly, all above elemetary row operations can be done using matrix multiplication between A and elemetary matrices. Simmilar as the way we think $AX$ is a linear combination of columns, we can think $XA$ is the linear combination of rows, where X is a row vector $1xm$, and the results of $XA$ is another row vector. For example: \n",
    "\n",
    "$AX = \\begin{bmatrix} 1 & 2 & 0 \\end{bmatrix}\\begin{bmatrix} 1 & 1 & 2 \\\\ -1 & 0 & 1 \\\\ 3 & 1 & 0 \\end{bmatrix} = 1* \\begin{bmatrix} 1 & 1 & 2 \\end{bmatrix} + 2*\\begin{bmatrix} -1 & 0 & 1 \\end{bmatrix} + 0*\\begin{bmatrix}  3 & 1 & 0 \\end{bmatrix}  = \\begin{bmatrix} -1 & 1 & 4 \\end{bmatrix} $\n",
    "\n",
    "By using the above idea, to convert $A = \\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 8 & 1 \\\\ 0 & 4 & 1  \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 2 & -2\\\\ 0 & 4 & 1   \\end{bmatrix} $, we basically keep row 1 and row 3 while subtract row 2 from 3 times of row 1. Thus, the elementary matrix $E_{21}$ to do this is: \n",
    "$ E_{21} = \\begin{bmatrix} 1 & 0 & 0 \\\\ -3 & 1 & 0 \\\\ 0 & 0 & 1  \\end{bmatrix}$.\n",
    "\n",
    "Simmilary to transform $\\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 2 & -2\\\\ 0 & 4 & 1   \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 2 & -2\\\\ 0 & 0 & 5  \\end{bmatrix}$,\n",
    "the elementary matrix $E_{32} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -2 & 1  \\end{bmatrix}$\n",
    "\n",
    "Above elementary matrices can be combined into a unique one: $ E = E_{32}E_{21} $\n",
    "\n",
    "In some other cases, sometimes we want to switch two rows. Let's say we want to switch row 1 and row 2 of A. The elementary matrix (permutation matrix) to achieve this is: \n",
    "$P_{32} = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1  \\end{bmatrix}$\n",
    "\n",
    "<h2> Lecture 3: Inverse of Matrices </h2>\n",
    "\n",
    "To find the inverse of matrix A, we can convert the joint matrix $[A, I]$ into $[I, R]$ using Gauss-Jordan method, then  $R =A^{-1}$. This is because when we transform $A$ into indentity matrix $I$, we use elementary matrix $E$: $EA = I$, thus E is $A^{-1}$. Additionally, we also multitply $E$ with $I$ to get $R=EI=E$, thus $R$ is exactly $E$ matrix.\n",
    "\n",
    " There are  cases that we can not find the invese A. In other words, A is not invertible:\n",
    " - A is not square matrix \n",
    " - columns of A is not independent.\n",
    " \n",
    "It is because for both cases $EA \\neq I$, thus $E$ is not $A^{-1}$\n",
    "\n",
    "<h2> Lecture 4: Factorization into A = LU </h2> \n",
    "We know that by multiply elemenatary matrix with A, we have $E_{32}E_{21}A = U$ \n",
    "U is echelon form. $A = E_{21}^{-1}E_{32}^{-1}U$\n",
    "Assume there's no row exchanges when peforming elemination, the matrix $L = E_{21}^{-1}E_{32}^{-1}$ is lower triangluar matrix. THis is because all elemetary matrices not for row exchanges are lower triangluar matrix, thus their inveserse and their multiplcations also have lower triangular form. For example, $ A = \\begin{bmatrix} 2 & 1\\\\ 8 & 7 \\end{bmatrix} = \\begin{bmatrix} 1 & 0\\\\ 4 & 1 \\end{bmatrix} \\begin{bmatrix} 2& 1\\\\ 0 & 3 \\end{bmatrix}$.Costs of elimination is $1/3n^3$.\n",
    "\n",
    "**Permutation Matrices**: used to perform row exchanges. In $3x3$ matrix, there's 6 matrices. $4x4$ : 12 matrices. \n",
    "An interesting thing about permutation matrix is $P^T = P^{-1}$.\n",
    "\n",
    "To deal with row changes in reality, matlab perform $PA =LU$ given A is invertible. it is not clear explained how P is calculated, but matlab picks pivots with big values for numerical accuracies.\n",
    "\n",
    "## Lecture 5/6: Vector Spaces:\n",
    "\n",
    "**Vector space**: consists of all vectors and their linear combinations. For example, $R^2$ consists of all vectors with 2 dimensional values. \n",
    "\n",
    "**Subspace**: subspaces of $R^2$ is original $[0;0]$, a line through origin, and all $R^2$. Subspaces of $R^3$ are: a plane (P) through origin, a line (L) through origin, origin, and all $R^3$. \n",
    "Rememeber all vector spaces and subspaces must contain origin points. The union of two subspaces is not necessary a subpace (e.g. $ P \\cup L $ is not a subspace). The intersection of two subspace (e.g. $ P \\cap L $) is a subspace.\n",
    "\n",
    "**Column space** $C(A)$: is a subspace formed by linear combinations of collumn vectors. For example:\n",
    " $ A = \\begin{bmatrix} 1 & 3\\\\ 2 & 3 \\\\ 4 & 1 \\end{bmatrix} $ has column space is a plane created by all linear combination of 2 vectors $[1,2,4]$ and $[3,3,1]$. This plane also go through origin in $R^3$. Column space is interesting because it tells us when $Ax = b$ can be solve. The system can be solved only $b$ is in the subspace of $A$\n",
    " \n",
    " **Null space**: N(A) is a subspace that contains all solutions $x$ of $Ax = 0$. For example,  $Ax = \\begin{bmatrix} 1 & 1 & 2\\\\ 2 & 1 & 3 \\\\ 3 & 1 & 4\\\\ 4 & 1 & 5\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}  = 0 $, $x = c  \\begin{bmatrix} 1\\\\ 1 \\\\ -1 \\end{bmatrix}$ is a line (a subspace) in $R^3$. Note that space containing solution $x$ for $Ax = b$ is not a subspace. This is because this space does not go through origin. \n",
    " \n",
    " ## Lecture 7: Solving $Ax = 0$, solve for the Null space $N(A)$\n",
    " \n",
    " As mentioned, to solve $Ax = 0$, we need to get to the echelon form of $A$, $U$ or reduced row echelon $R$.\n",
    " \n",
    " $ A = \\begin{bmatrix}  1 & 2 & 2 & 2\\\\ 2 & 4 & 6 & 8 \\\\ 3 & 6 & 8 & 10 \\end{bmatrix} \\rightarrow U = \\begin{bmatrix} 1 & 2 & 2 & 2 \\\\ 0 & 0 & 2 & 4 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\rightarrow R = \\begin{bmatrix} 1 & 2 & 0 & -2 \\\\ 0 & 0 & 1 & 2 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$ \n",
    " \n",
    "**rank of matrix** is the number of pivot columns or the number of independent cols. In this example, rank is 2.\n",
    " \n",
    "**free cols or free variable** is $ m = r$. In this example, there are two free cols. \n",
    "\n",
    "**specical solutions**. The number of special solutions are the number of free cols. This is because for each free variable\n",
    " we can assign any free value. For example, in this example, $x_2$ and $x_4$ are free variables. We can assign $x_2 = 1, x_4 =0$ or $x_2 = 0, x_4 =1$ to get the following special solutions. The final solution is the combination of these special solutions.\n",
    " \n",
    " $N(A)= c \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}  + d \\begin{bmatrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix} $\n",
    " \n",
    " Note the reduced row echelon form $R$ even brings us closer to the solutions. \n",
    " \n",
    " ## Lecture 8: Solve $Ax = b$ and what rank of matrix tells us ?\n",
    " \n",
    " Again, $Ax = b$ is solvable if b is in column space of A. One thing we can notice that if a combination of rows of A\n",
    " gives a 0 row then the same combination of components of b is 0\n",
    " \n",
    " Assume $Ax = b $ is solvable. We can solve it in a simmilar way of solving $Ax =0$. However, the complete solution of \n",
    " $ Ax =b $ is: \n",
    " \n",
    " $ x_{complete} = x_{particular} + x_{n}$, where $x_{particular}$ can be found be assign 0 to free variables and calculate pivot variables. $x_{n}$ is the solutions of $Ax = 0$.\n",
    " \n",
    " For example, let's solve $Ax = \\begin{bmatrix}  1 & 2 & 2 & 2\\\\ 2 & 4 & 6 & 8 \\\\ 3 & 6 & 8 & 10 \\end{bmatrix} x = \\begin{bmatrix} 1 \\\\5 \\\\6 \\end{bmatrix}$. Echelon form is: $Ax = \\begin{bmatrix}  1 & 2 & 2 & 2 \\\\ 0 & 0 & 2 & 4 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} x = \\begin{bmatrix} 1 \\\\3 \\\\ 0 \\end{bmatrix}$. \n",
    " \n",
    "Set free variables to 0, we get particular solution:\n",
    "$x_{particular} = \\begin{bmatrix} -2 \\\\ 0 \\\\ 3/2 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "From lecture 7, $x_{n} = c \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}  + d \\begin{bmatrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix} $. Thus, $x_{complete} = x_{particular} + x_{n} = \\begin{bmatrix} -2 \\\\ 0 \\\\ 3/2 \\\\ 0 \\end{bmatrix} + c \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}  + d \\begin{bmatrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix} $\n",
    "\n",
    "\n",
    "**Rank of matrix tells us about number of solutions of** $Ax =b$. \n",
    "Assume A is a matrix of m rows and n cols. We know that rank $r \\leq m $ and $ r \\leq n$, because\n",
    "each row or each col has maximum 1 pivot. There are 4 cases: \n",
    "\n",
    "- $r = m = n$. $A$ is full rank of row and col. All cols are linearly independent. Reduced row matrix $R=I$. Thus, $Ax=b$ has only one solutions for every b. Note that only in this case, A is invertible.\n",
    "- $r = m < n$. $A$ is full row rank. A has pivots every rows. $ R = [I,F]$, $F$ and $I$ can be mixed. Thus, it always has infinte solutions.\n",
    "- $r = n < m$. $A$ is full col rank. A has pivots every cols. $ R = \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}$. Thus, $Ax =b $ has 0 or 1 solution depending on whether b belongs to column space of A or not. \n",
    "- $r < m$ and $r < n$.  $R = \\begin{bmatrix} I & F \\\\ 0 & 0 \\end{bmatrix} $. It has 0 or infinite solutions depending on whether b is in columns space of A or not. \n",
    "\n",
    "## Lecture 9: Independence, Basis and Dimension. \n",
    "\n",
    "- Vectors $v_1, v_2, ..., v_n$ are independent if there is no combinations of them give zero vector (except zero combinations). How to check that? we can solve $Ax = 0$, with $ A = [v_1, v_2, ..., v_n]$ and make sure that this equation has only solution is x = 0 or nullspace of A $N(A)$ is zero vector. To achieve this, $rank(A) = n$. Note that the number of rows could be $m > n$, because from Lecture 8, $N(A)$ is always zero vector. Also note that, it just say now the way to check the indepences of vectors, not the spanning of these vectors. \n",
    "- If these vectors are both indenpent and span the spaces, there are basic vectors. The only way these vectors are basic vectors is that $ A = [v_1, v_2, ..., v_n]$ has $rank = m = n$, because if $n < m$ (more rows than cols), these vectors do not span the whole space of A (they span the column subspace of A only). \n",
    "- The $rank(A) = #pivots = #dimension of column space$ \n",
    "- Dimension of $N(A) = #free cols$.\n",
    "- Note that there infinite number of basic vectors as long as they are indenpendent and span the space. \n",
    "\n",
    "## Lecture 10: The Four Fundamental Subspaces.\n",
    "\n",
    "We need to know what basis of following spaces and the dimension of them. \n",
    "- **Column space** $C(A)$: basis vectors are pivots cols, dimension is rank(A).\n",
    "- **Null space** $N(A)$: basis vectors are special solutions of $Ax = 0$, dimension n - rank(A).\n",
    "- **Row Space** $C(A^T)$ basis vectors are first pivots rows, dimension is rank(A).\n",
    "- **Null space of** $A^T$: basic vectors are sepcial solutions of $A^Ty =0$ dimension is m - rank(A). Because \n",
    "$A^Ty = y^TA = 0$. Thus, we implicitly can find special solution $y^T$ in elementary matrix $E$, which are rows y of $E$\n",
    "that makes $y^TA = 0$. \n",
    " \n",
    " Note that, row elimination, which results reduce row echelon form, change the collumns space but row space stay the same. \n",
    "\n",
    " \n",
    "## Lecture 11: Matrix Spaces, Rank 1 matrix\n",
    "## Lecture 12: Graphs, Networks, Incidence Matrices.\n",
    "## Review \n",
    "## Lecture 14: Orthorgonal vectors and spaces.\n",
    "\n",
    "**Definition** Subpace S is orthorgonal with subspace T means every vectors in S is orthogonal with every vector in T.\n",
    "\n",
    "So we can see that the row space $C(A^T)$ is orthogonal with Nullspace $N(A)$. This is because the nullspace if exists sastifies $Ax = 0$ (every row in A:$row_i$&x = 0). The columnspace $C(A)$ is orthogonal with nullspace of $A^T$ because \n",
    "$A^Ty = 0$. \n",
    "\n",
    "\n",
    "## Lecture 15: Projections onto subspace.\n",
    "\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec 15 p1.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec 15 p2.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec 15 p3.png\" width=\"800\">\n",
    "\n",
    "## Lecture 16: Least Squares\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec16 p1.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec16 p2.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec16 p3.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec16 p4.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## Lecture 17: Orthogonal Matrices and Gram-Schmidt\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec17 p1.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec17 p2.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec17 p3.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec17 p4.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec17 p5.png\" width=\"800\">\n",
    "\n",
    "## Lecture 18: Properties of Determinants.\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec18 p1.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec18 p2.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec18 p3.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec18 p4.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec18 p5.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec18 p6.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## Lecture 19: Determinant Formulas and Cofactors.\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec19 p1.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec19 p2.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec19 p3.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec19 p4.png\" width=\"800\">\n",
    "<img src=\"./images/Linear Algebra MIT 2005 lec19 p5.png\" width=\"800\">\n",
    "\n",
    "## Lecture 20: Cramer's Rule, Inverse Matrix, and Volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
